{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irfan270791/CAPSTONE-PROJECT-2--Regression---Appliance_Energy_Prediction/blob/main/IM_Appliance_Energy_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -   Appliance_Energy_Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -**  - Irfan Momin\n",
        "##### **Team Member 2 -**  - Sushil Ghodwinde\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Energy is critical to economic and social growth.It enables nations to achieve their aim of higher living conditions.\n",
        "\n",
        "So,these days,planning and oprating energy production and consumption is a must.Understanding how energy is used is necessary for better management.This presented an opportunity to create a supervised machine learning method to forecast Appliance Energy use.\n",
        "\n",
        "The initial phase,based on our dataset,was data prepressing.we understood the data and discovered that there are no null values in the dataset, and we received a detailed description of the characteristics involved.\n",
        "\n",
        "Followig that,Exploratory data Analysis and Data Visualisation produced a concise knowledge of the link between features and label,i.e., the dependent variable it has also given an idea of the features to be chosen for the next step.The heatmap was utilised to understand the association between independent variables, based on which important features were chosen.Choosing the right elements to improve accuracy was difficult.\n",
        "\n",
        "Standardisation was critical step before fitting the model since it ensure that the feature values in the data had a zero mean and unit variance.When we construct any Machine Learning method, there is chance that the objectie function will not perform properly unless it is normalised.\n",
        "\n",
        "Following completion of training and assessment.Linear,Lasso,Ridge,Elasticnet, Gradient Boosting, Random Forest and XGBoosting techniques were employed.We checked and compared numerous matrices and determined that the Random Forest Regressor produces the best results.\n",
        "\n",
        "We used the ExtraTreeRegressor and used Hyperparameter Tuning to improve accuracy and prevent overfitting.We also used Gridsearch cross validation to find the optimal parameter.This parameter improved our model's prediction performance.Finally we performed Model Explainability using ELI5, which assisted us in understanding the participation of features and their impact on the target variables.To summarise, the ExtraTreeRegressor has proven to be the best model for our dataset."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Irfan270791/CAPSTONE-PROJECT-2--Regression---Appliance_Energy_Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For approximately 4.5 months,the dataset is set to 10 minutes.A ZigBee wireless sensor network was used to monitor the house's temperature and humidity levels.Every wireless node reported the temperature and humidity levels every 3.3 minutes.The wireless data was then averaged over 10 minutes periods.The energy data was recorded every 10 minutes using m-bus energy meters. Weather from the nearest airport weather station  (Chievres Airport, Belgium) was collected from a public data set from reliable prognosis(rp5.ru) and blended with the experimental data set using the data and time columns.Two random variable were included in the dataset to test the regression models and to filterout non-predictive features(parameters).\n",
        "\n",
        "The problem statment is to create a machine learning model that can accurately forecast energy usage based on the supplied features.This might be valuable for building managers,energy firm and policymakers who need to optimise energy consumption, cut costs and minimise the environmental impact of energy usage.\n",
        "\n",
        "Specifically , the model should be able to reliably anticipate energy usage based on the different elements that influence energy consumption,such as temperature, humidity, illumination and time of the day.This can assist building managers and energy firms in identifying patterns and trends in energy consumption and making informed energy decisions, such as altering HVAC settings, optimising lighting or introducing energy-efficient solutions.Policymakers can also utilise this data to create regulations and incentives that encourage energy efficiency and sustainability."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "%matplotlib inline\n",
        "\n",
        "# Import datetime library.\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "# Library of warning would assist in ignoring warning issued.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import necessary statistical libraries.\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Import library for ML-Model.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, RandomForestRegressor, StackingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Library for save the model.\n",
        "import pickle"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D2qLoSEE6_FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "filepath = '/content/drive/MyDrive/Appliances energy consumption/data_application_energy.csv'\n",
        "df_energy = pd.read_csv(filepath)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df_energy.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df_energy.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of rows {df_energy.shape[0]}, and number of columns are {df_energy.shape[1]}\")"
      ],
      "metadata": {
        "id": "U0rZbowj8V-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df_energy.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* We have 29 Columns and maximum columns are numerical except date.\n",
        "* we have to convert the date column into datetime format.\n",
        "\n"
      ],
      "metadata": {
        "id": "OM0Q5BdL9Eb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How Important is it to get rid of duplicate records in our data?**\n",
        "\n",
        "When there are identical instances in a dataset, it is referred to as \"duplication\". Such duplication could occur due to erroneous data entry or data collection procedures.Eliminating duplicate data from the dataset saves time and money by avoiding the repetition of the same data sent to machine learning model."
      ],
      "metadata": {
        "id": "KK5UwG8Y9yki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "value = len(df_energy[df_energy.duplicated()])\n",
        "print(\"The number of duplicate values in the dataset is =\",value)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that there is no duplicate entry in the above data."
      ],
      "metadata": {
        "id": "osbB5KyXAPf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why dealing with missing values is necessary."
      ],
      "metadata": {
        "id": "boS6PELhAsKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerous missing values are frequently present in real-world data, Which may be the result of data corruption or another issue. In order to handle missing values during the dataset pre-processing step, as many machine-learning algorithms do not support them.Consequently, identifying the missing values is the first step in dealing with missing data."
      ],
      "metadata": {
        "id": "U87RYBz3A-Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df_energy.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.bar(df_energy, color='green', sort='ascending', figsize=(15,5), fontsize=15)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above command and figure, we noticed that every column has 0 null values. This seems to be clean data and there is no missing data in any of the rows and columns."
      ],
      "metadata": {
        "id": "VcJ6nhV_Dcc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The dataset provided contains 29 columns and 19735 rows and does not have any missing or duplicate values.\n",
        "* The goal is to predict the energy use of appliances.Demand prediction involves analytical studies on the probability of house temperature and humidity conditions, which were monitored with a ZigBee wireless sensor network for 10 mins for about 4.5 months.\n",
        "* There are no duplicate Columns.\n",
        "* There are no missing values.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df_energy.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df_energy.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date time year - Month-Day hour:Minute:Second.\n",
        "\n",
        "Appliances - Energy use in Wh.\n",
        "\n",
        "Lights - Energy use of light fixtures in the house in Wh.\n",
        "\n",
        "T1 - Temperature in kitchen area, in celsius.\n",
        "\n",
        "RH_1 - Humidity in kitchen area, in %.\n",
        "\n",
        "T2 - Temperature in living room area, in celsius.\n",
        "\n",
        "RH_2 - Humidity in living room area, in %\n",
        "\n",
        "T3 - Temperature in laundry room area.\n",
        "\n",
        "RH_3 - Humidity in laundry room area, in %\n",
        "\n",
        "T4 - Temperature in office room, in celsius.\n",
        "\n",
        "RH_4 -  Humidity in office room area, in %\n",
        "\n",
        "T5 - Temperature in bathroom, in celsius.\n",
        "\n",
        "RH_5 - Humidity in bathroom area, in %\n",
        "\n",
        "T6 - Temperature outside the building (north-side), in celsius.\n",
        "\n",
        "RH_6 - Humidity in outside the building (north-side), in %\n",
        "\n",
        "T7 - Temperature in ironing room, in celsius.\n",
        "\n",
        "RH_7 - Humidity in ironing room, in %\n",
        "\n",
        "T8 - Temperature in teenager room 2, in celsius.\n",
        "\n",
        "RH_8 - Humidity in teenager room 2, in %\n",
        "\n",
        "T9 - Temperature in parents room , in celsius.\n",
        "\n",
        "RH_9 - Humidity in parents room, in %\n",
        "\n",
        "To - Temperature outside (from chievres weather station), in celsius.\n",
        "\n",
        "Pressure (from chievres weather station) - in mm Hg.\n",
        "\n",
        "RH_out - Humidity outside (from chievres weather station), in %\n",
        "\n",
        "Wind speed (from chievres weather station) - in m/s\n",
        "\n",
        "Visibility (from chievres weather station) - in km\n",
        "\n",
        "Tdewpoint (from chievres weather station) - Â°C\n",
        "\n",
        "rv1 - Random variable 1, nondimensional.\n",
        "\n",
        "rv2 - Random variable 2, nondimensional.\n",
        "\n",
        "Where indicated, hourly data(then interpolated) from the nearest airpot\n",
        "weather station (Chievres Airpot, Belguim) was downloaded from a public data set from reliable prognosis ,rp5.ru. Permission was obtained from reliable prognosis for the distribution of the 4.5 months of weather data."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df_energy.columns.tolist():\n",
        "  print(\"No of Unique values in \", i,\"is\",df_energy[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Renaming the columns according to the Variable Description."
      ],
      "metadata": {
        "id": "UqT-3pcP4mVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the Columns.\n",
        "df_energy.rename(columns={'T1':'temp_kitchen','RH_1':'hu_Kitchen','T2':'temp_living_room','RH_2':'hu_living','T3':'temp_Laundry_room',\n",
        "                          'RH_3':'hu_laundry','T4':'temp_office_room','RH_4':'hu_office','T5':'temp_bathroom','RH_5':'hu_bath','T6':'temp_build_out',\n",
        "                          'RH_6':'hu_build_out','T7':'temp_ironing_room','RH_7':'hu_ironing_room','T8':'temp_teen_room','RH_8':'hu_teen',\n",
        "                          'T9':'temp_parents_room','RH_9':'hu_parent','T_out':'temp_out','RH_out':'out_humidity'},inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the data format of date column.\n",
        "df_energy['date']=pd.to_datetime(df_energy['date'])"
      ],
      "metadata": {
        "id": "cHoFeNSZ_iaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the months and days from date.\n",
        "df_energy['month'] = df_energy['date'].dt.month\n",
        "df_energy['weekday'] = df_energy['date'].dt.weekday\n",
        "df_energy['hour']  = df_energy['date'].dt.hour"
      ],
      "metadata": {
        "id": "KRXW4yO2AHUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the date columns.\n",
        "df_energy.drop('date',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Jc9ZyibsBJ33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the column list for better analysis.\n",
        "temp_cols = ['temp_kitchen','temp_living_room','temp_Laundry_room','temp_office_room','temp_bathroom','temp_build_out','temp_ironing_room','temp_teen_room','temp_parents_room']\n",
        "hu_cols = ['hu_Kitchen','hu_living','hu_laundry','hu_office','hu_bath','hu_build_out','hu_ironing_room','hu_teen','hu_parent']\n",
        "light_cols = ['light']\n",
        "weather_cols = ['temp_out','out_humidity',\"Tdewpoint\",\"Press_mm_hg\",\"Windspeed\",\"Visibility\"]\n",
        "date_col = ['month','weekday','hour']\n",
        "random_col = [\"rv1\",\"rv2\"]"
      ],
      "metadata": {
        "id": "nIzqB8G8BceO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at first 5 rows after rename the columns.\n",
        "df_energy.head()"
      ],
      "metadata": {
        "id": "lIrbuuw0E2qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Splitting date columns into hour,weekdays,months.\n",
        "2. it will help in analysing the data on the bases of months,days and hours.\n",
        "3. Dropped the date column due to no further use of them.\n",
        "4. Renaming some features, for better understanding and readibility.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1  Appliance Energy Column."
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig,ax = plt.subplots(2,2,figsize=(15,10))\n",
        "\n",
        "# Distribution of Appliances.\n",
        "dist = sns.distplot(df_energy['Appliances'],ax=ax[0,0])\n",
        "dist.set_title('Distribution of Appliances Energy')\n",
        "\n",
        "# Average Appliances Energy over month.\n",
        "month_eng = pd.DataFrame(df_energy.groupby('month')['Appliances'].mean()).reset_index()\n",
        "sns.violinplot(x= df_energy['month'], y=df_energy['Appliances'], ax=ax[0,1])\n",
        "\n",
        "# Average Appliances Energy over weekdays.\n",
        "weekday_eng = pd.DataFrame(df_energy.groupby('weekday')['Appliances'].mean()).reset_index()\n",
        "sns.boxplot(x = df_energy['weekday'], y=df_energy['Appliances'], ax=ax[1,0])\n",
        "\n",
        "# Average Appliances Energy over hours.\n",
        "hour_eng = pd.DataFrame(df_energy.groupby('hour')['Appliances'].mean()).reset_index()\n",
        "sns.barplot(x = hour_eng['hour'], y= hour_eng['Appliances'], ax=ax[1,1])"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The reason for using the distplot is that it provide a quick overview of the skewness, allowing us to decide whether or not to perform the transformation.\n",
        "* Violin plots depict summary statistics and the density of each variable.\n",
        "* Box plots are used to show distribution of numeric data values and compare them between multiple groups.\n",
        "* Bar plots enables to compare categorical data in dependency of numerical data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Appliances Energy has positive skewness. A log transformation can make it normal. Most of the values are around 100 wh. Outliers are also present in column.\n",
        "* In the month of January, the energy usage is high compared to other months, and February has low energy consumption.\n",
        "* On Thursday and Saturday, the energy usage is high compared to other days, and Tuesday has low energy consumption.\n",
        "* In the hours of the day, 8hrs to 21hrs have high energy consumption.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the hours of the day from 10PM TO 7PM, we can save the most energy and send the excess power to the grid.In the weeks of Tuesday and February, we can save the most money on power usages"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2   Temperature columns(Univariate + Bivariate)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "n = len(temp_cols)\n",
        "fig,ax=plt.subplots(len(temp_cols),2,figsize=(20,40))\n",
        "for i,col in enumerate(temp_cols):\n",
        "  # Univariate of the columns.\n",
        "  dist = sns.distplot(df_energy[col],ax=ax[i,0])\n",
        "  ax[i,0].axvline(df_energy[col].mean(),color='orange',linestyle= 'dashed', linewidth=2)\n",
        "  ax[i,0].axvline(df_energy[col].median(),color='green',linestyle= 'dashed', linewidth=2)\n",
        "  # Bivariate Analysis the Appliance Energy.\n",
        "  # lineplot.\n",
        "  scatter=sns.lineplot(data=df_energy, x=col, y='Appliances', color='green',ax=ax[i,1]);"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The reason for using the distplot is that it provides a quick overview of the skewness, allowing us to decide whether or not to perform the transformation.\n",
        "* A Scatter plot is used to plot how much a numerical feature is affected by another numerical value.\n",
        "* A lineplot is also used to observe how numerical values change over time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* All Temperature columns are  followed normally distributed except parents rooms temperature.\n",
        "* Inside building mean and median value of all room temperature lies between 19 to 22 degree celcius.\n",
        "* Outside building mean and median value of temperature lies between 6 to 7 degree celcius.\n",
        "* The lines in nearly all of the columns follow the same pattern.\n",
        "* It could be due to the same type of relationship between temperature variables and the Appliances Energy Consumption.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* When inside building temperatures are below 18 degree celcius energy consumption is minimum means maximum energy can saved.\n",
        "* All temperatures have similar relationship with appliance energy consumption. it leads to multicolinarity. These columns will be dropped in upcoming appropriate section.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3  Humidity columns (Univariate + Bivariate)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig,ax=plt.subplots(len(hu_cols),2,figsize=(20,40))\n",
        "for i,col in enumerate(hu_cols):\n",
        "  # Univariate of the columns.\n",
        "  dist= sns.distplot(df_energy[col],ax=ax[i,0])\n",
        "  ax[i,0].axvline(df_energy[col].mean(), color='orange', linestyle= 'dashed', linewidth=2)\n",
        "  ax[i,0].axvline(df_energy[col].median(), color='green', linestyle= 'dashed', linewidth=2)\n",
        "  # Bivariate Analysis the Appliance Energy.\n",
        "  # lineplot.\n",
        "  scatter= sns.scatterplot(data=df_energy, x=col, y='Appliances', color='green', ax=ax[i,1]);"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The reason for using the distplot is that it provides a quick overview of the skewness, allowing us to decide whether or not to perform the transformation.\n",
        "* A Scatter plot is used to plot how much a numerical feature is affected by another numerical value.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* All humidity distribution is followed normal distribution except outside building humidity.\n",
        "* In inside building the mean and median value of humidity distribution lies between 35 to 45 precent,except bathroom humidity and it is near about 50 precent.\n",
        "* For outside building humidity, the mean and median value is around 55 precent.\n",
        "* For inside building low and high humidity causes low consumption of appliances energy. otherwise it causes high spreading data of energy consumption.\n",
        "* For outside building humidity energy consumption data is widely spread irresective of humidity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Low humidity and high humidity can saves maximum energy.\n",
        "* All humidities have similar relationship with appliance energy consumption. it leads to multicolinarity. These columns will be dropped in upcoming appropriate section.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4   Weather Columns(Univariate + Bivariate)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fig,ax=plt.subplots(len(weather_cols),2,figsize=(20,30))\n",
        "for i,col in enumerate(weather_cols):\n",
        "  # Univariate of the columns.\n",
        "  dist=sns.distplot(df_energy[col],ax=ax[i,0])\n",
        "  ax[i,0].axvline(df_energy[col].mean(), color='orange', linestyle='dashed',linewidth=2)\n",
        "  ax[i,0].axvline(df_energy[col].median(), color='green', linestyle='dashed', linewidth=2)\n",
        "  # Bivariate analysis the Appliance Energy.\n",
        "  # lineplot.\n",
        "  scatter = sns.regplot(data=df_energy, x=col, y='Appliances',color='orange',ax=ax[i,1]);"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The reason for using the distplot is that it provides a quick overview of the skewness, allowing us to decide whether or not to perform the transformation.\n",
        "* Regplot method is used to plot data and a linear regression model fit. we can observe if the feature can fit with regression problem.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Temprature outside,presure,Tdewpoint have followed normal distribution.\n",
        "* Windspeed have positively skewed.\n",
        "* Visibility, outside humidity are negatively skewed.\n",
        "* When outside temperature is below 0 degree the spred of energy used data is low. The mean value of energy used is exceptionally high near -3 and 22 degree celcius.\n",
        "* When outside humidity is below 40 percent ,spred of energy used data is low and it is near about gradually increase with increase the humidity. The mean value of energy used is exceptionally high near about 30 and 60 percent of humidity.\n",
        "* When presure is below 745 hg,distribution of energy consumption low,other than it is highly spred.\n",
        "* When wind-speed is below 1m/s and above 12m/s the distribution of energy consumption is low. when wind-speed is 4m/s and 9m/s the mean energy consumption is exceptionally high.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* When the outside temperature is below 0 degree celcius, outside humidity is below 40 percent, visibility is below 10 and Tdewpoint is between 0-5 the posibility of energy saving is high.\n",
        "* Temperature, Humidity,Tdewpoint has high probability of colinearity. it will inspect further in upcoming section.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5  Light Column  (Univariate + Bivariate)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "fig, ax=plt.subplots(1,2,figsize=(15,5))\n",
        "# Univariate of columns.\n",
        "dist= sns.distplot(df_energy['lights'],ax=ax[0])\n",
        "ax[0].axvline(df_energy[col].mean(), color='orange', linestyle= 'dashed', linewidth=2)\n",
        "ax[0].axvline(df_energy[col].median(), color='green', linestyle= 'dashed', linewidth=2)\n",
        "# Baivariate Analysis the Appliance Energy.\n",
        "# Lineplot.\n",
        "scatter = sns.scatterplot(data= df_energy, x= 'lights', y= 'Appliances', color= 'orange', ax=ax[1]);"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The reason for using the distplot is that it provides a quick overview of the skewness, allowing us to decide whether or not to perform the transformation.\n",
        "* A Scatterplot is used to plot how much a numerical feature is affected by another numerical value."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This columns neither follows the normal distribution nor the relevant skewness, Also because this column isn't adding many values to the dataset. We'll remove it in the upcoming step.  "
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has no impact on the business."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6   Random variable Column  (Univariate + Bivariate)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig,ax=plt.subplots(len(random_col),2,figsize=(15,10))\n",
        "for i,col in enumerate(random_col):\n",
        "  # Univariate of the columns.\n",
        "  dist = sns.distplot(df_energy[col], ax=ax[i,0])\n",
        "  ax[i,0].axvline(df_energy[col].mean(), color= 'orange', linestyle= 'dashed', linewidth=2)\n",
        "  ax[i,0].axvline(df_energy[col].median(), color= 'green', linestyle= 'dashed', linewidth=2)\n",
        "  # Bivariate Analysis the Appliance Energy.\n",
        "  # lineplot.\n",
        "  scatter = sns.lineplot(data= df_energy, x=col, y='Appliances', color= 'orange', ax=ax[i,1]);\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The reason for using the distplot is that it provides a quick overview of the skewness, allowing us to decide whether or not to perform the transformation.\n",
        "* A lineplot is also used to observe how numerical values change over time."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Distribution of Random variable are symmetric.\n",
        "* Two Random variable has same distribution and similar relationship with appliance energy. it leads to multicolinearity and one should must be dropped in upcoming step.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has no impact on business."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7   Temperature VS Humidity."
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Creating list of temperature and humidity.\n",
        "temperature_col = temp_cols.copy()\n",
        "temperature_col.append('temp_out')\n",
        "humidity_col = hu_cols.copy()\n",
        "humidity_col.append('out_humidity')\n",
        "# zip the list.\n",
        "temp_hum = list(zip(temperature_col,humidity_col))"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting the temperature humidity relationship.\n",
        "fig,ax=plt.subplots(len(temp_hum),1,figsize=(15,35))\n",
        "for i,cols in enumerate(temp_hum):\n",
        "  sns.regplot(x=df_energy[cols[0]], y= df_energy[cols[1]],ax=ax[i])"
      ],
      "metadata": {
        "id": "v4IaDkruT9Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regplot method is used to plot data and a linear regression model fit. we can observe if the feature can fit with regression problem."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Outside temperature and humidity, teen temprature and humidity, outside\n",
        "building temprature and humidity, living room temperature and humidity has strong negative linear relationship.\n",
        "* Others temperature humidity relationship has mild negative linear relationship."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These relationships might be leads to multicolinearity."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8   Correlation Heatmap"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(25,15))\n",
        "correlation = df_energy.corr()\n",
        "sns.heatmap(correlation, annot= True, cmap= 'coolwarm', linewidths=0.8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main reasons for using Heatmap are :\n",
        "\n",
        "* Heatmaps provide a quick overview of critical web performance factors.\n",
        "* Heatmaps are a visual way to understand numerical features.\n",
        "* Heatmaps assist businesses in making informed decisions that benefit the bottom line.\n",
        "* Its also aids in comprehending multicolinearity, which allows us to determine which features to keep and which to eliminate or combine.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Temprature columns - it is clear from the heatmap that four columns have a high degree of correlation with parent_room_temperature that are temp_laundry_room, temp_bathroom, temp_ironing_room, temp_teen_room also temp_buid_out & temp_out has high correlation. Hence temp_build_out & temp_parent_room is to be removed from training set as information provided by them can be provided by other features.\n",
        "* Humidity column - For each and every humidity columns, we see moderate correlation which is workable.\n",
        "* Weather_column - Visibility,Tdewpoint,Press_mm_hg have low correlation values.\n",
        "* Random variable column - Similar to the trend that we have seen in the weather columns we have low correlations.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9  Pair Plot of Temperature Columns."
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 Pair Plot visualization code\n",
        "plt.figure(figsize=(25,25))\n",
        "pair=sns.pairplot(df_energy[temp_cols],diag_kind= 'kde')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a data visualization that plot pair-wise relationship between all the variables in a dataset.This helps to better understand the relationship visually."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* All Temperature has linear relationship with each other.\n",
        "* temperature of parents_room with laundary_room,bathroom,ironing_room, teen_room temperature has strong linear relationship.\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10  pair plot of Humidity column."
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 Pair plot visualization code\n",
        "plt.figure(figsize=(25,25))\n",
        "pair = sns.pairplot(df_energy[hu_cols], diag_kind = 'hist')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a data visualization that plot pair-wise relationship between all the variables in a dataset.This helps to better understand the relationship visually."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* All humidity columns have linear relationship with each other except outside building humidity and bathroom relationship.\n",
        "* Humidity column may be lead to multicolinearity. it will be treated upcoming step.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11  Pair plot of Weather Column."
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 Pair plot visualization code.\n",
        "plt.figure(figsize=(25,25))\n",
        "pair= sns.pairplot(df_energy[weather_cols], diag_kind='kde')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a data visualization that plot pair-wise relationship between all the variables in a dataset.This helps to better understand the relationship visually."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Temperature and Humidity has negative linear relationship.\n",
        "* Temperature and Tdewpoint has positive linear relationship.\n",
        "* Temperature may be lead to multicolinearity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 -"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 -"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. There is no change in appliance energy consumption on weekdays and weekend.\n",
        "2. There is no significant difference in the energy consumption for appliances between day and night.\n",
        "3. The mean temperature in kitchen is greater than normal room temperature.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis** - There is no change in appliance energy consumption on weekdays and weekend.\n",
        "\n",
        "**Alternate Hypothesis**  - There is high appliances energy consumption on weekends as compared to weekdays."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "data_weekday = df_energy[df_energy['weekday'] <=5][['Appliances']]\n",
        "data_weekend = df_energy[df_energy['weekday'] > 5][['Appliances']]\n",
        "\n",
        "# Statistics Test and P-value.\n",
        "t_stat, p_val = stats.ttest_ind(data_weekday, data_weekend, equal_var=True)\n",
        "\n",
        "print('T-Statistics value', t_stat)\n",
        "print(\"P-value\", p_val)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Independent two-sample t-test is the statistical test used to calculate the P-value here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Specific statistical test was chosen because it is appropriate to compare the means of two separate set of numerical data as is the case in this situation, Which compares overall consumption on weekdays against weekend. Given that the data is continuous and regularly distributed, the t-test assumes that it follows a normal distribution. The equal_var parameter is set to True, assuming that variances of the two groups are equal."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis **  - There is no significant difference in the energy consumption for appliances between day and night.\n",
        "\n",
        "**Alternate Hypothesis** - There is a significant difference in the energy consumption for appliances between day and night."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "data_day = df_energy[(df_energy['hour'] >= 6) & (df_energy['hour'] < 18)][['Appliances']]\n",
        "data_night = df_energy[(df_energy['hour'] < 6) | (df_energy['hour'] >=18)][['Appliances']]\n",
        "\n",
        "# Statistics Test and P-value.\n",
        "t_stat, p_val = stats.ttest_ind(data_day, data_night, equal_var=True)\n",
        "\n",
        "print('T-Statistics Value', t_stat)\n",
        "print(\"P-Value\", p_val)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Independent two-sample t-test is the statistical test used to calculate the P-value here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical test was chosen because it is appropriate to compare the mean of two separate set of numerical data, as is the case in this situation, which compares overall consumption on weekday against weekends. Given that the data is continuous and regularly distributed, the t-test assumes that it follows a normal distribution. From the parameter we can say that there is a significant difference in the energy consumption for appliances between day and night."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis** - The mean temperature in kitchen is greater than normal room temperature.\n",
        "\n",
        "**Alternate Hypothesis** - The temperature in the kitchen is at max room temperature and it can not be above it."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the kitchen temperature.\n",
        "kitchen_rand = df_energy['temp_kitchen'].sample(1000)\n",
        "N=len(kitchen_rand)\n",
        "# Mean of the sample.\n",
        "kitchen_rand_mean = kitchen_rand.mean()\n",
        "# Normal room temperature.\n",
        "nrt = 20\n",
        "# The standard deviation for population.\n",
        "std_pop = df_energy['temp_kitchen'].std()\n",
        "\n",
        "# Perform Statistical test to obtain P-value.\n",
        "Z_stat = ((kitchen_rand_mean - nrt)/(std_pop/np.sqrt(N)))\n",
        "print(f'Z_score is {Z_stat} ')\n",
        "\n",
        "P_value= norm.cdf(Z_stat,0,1)\n",
        "print(f'P_value is  {P_value} ')\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the normal cumulative distribution function with the mean and standard deviation of a random sample to calculte the Z statistics for a particular proportion and then find its P_value."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z Statistics provide precise value for testing the hypothesis. it is easily comparable to its critical levels. According to the results null hypothesis can rejected."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df_energy.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is null values in Dataset.As a result, no imputation is required."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating copy of dataset.\n",
        "df = df_energy.copy()\n",
        "col_list = list(df.describe().columns)"
      ],
      "metadata": {
        "id": "fSFygZ-3axjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the Columns have Outliers with boxplot.\n",
        "plt.figure(figsize=(25,20))\n",
        "plt.suptitle(\"Box Plot\", fontsize = 18, y= 0.95)\n",
        "\n",
        "for n, ticker in enumerate(col_list):\n",
        "  ax = plt.subplot(8, 4, n+1)\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "  sns.boxplot(x=df[ticker], color='g', ax=ax)\n",
        "  # Chart formatting.\n",
        "  ax.set_title(ticker.upper())"
      ],
      "metadata": {
        "id": "UaWA5VI4bIqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Majority of Appliance use between 0 to 200 wh of energy.it is apparent that many variable contain outliers.\n",
        "\n",
        "Outliers can be found in all temperature columns.\n",
        "\n",
        "Outliers also exist in all humidity columns,Windspeed,Tdewpoint,Visibilty and the variable of interest outliers exist in appliances as well."
      ],
      "metadata": {
        "id": "5tv2shoGjVbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "for ftr in col_list:\n",
        "  print(ftr,'\\n')\n",
        "  q_25 = np.percentile(df[ftr],25)\n",
        "  q_75 = np.percentile(df[ftr],75)\n",
        "  iqr = q_75 - q_25\n",
        "  print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q_25,q_75,iqr))\n",
        "  # Calculate the outlier cutoff.\n",
        "  cut_off = iqr * 1.5\n",
        "  lower = q_25 - cut_off\n",
        "  upper = q_75 + cut_off\n",
        "  print(f\"\\nlower = {lower}  and upper = {upper} \\n\")\n",
        "  # identify outliers.\n",
        "  outliers = [x for x in df[ftr] if x < lower or x > upper]\n",
        "  print('Identified outliers: %d' % len(outliers))\n",
        "  # Removing Outliers.\n",
        "  if len(outliers) !=0:\n",
        "    def bin(row):\n",
        "      if row[ftr]> upper:\n",
        "        return upper\n",
        "      if row[ftr] <lower:\n",
        "        return lower\n",
        "      else :\n",
        "        return row[ftr]\n",
        "\n",
        "\n",
        "\n",
        "    df[ftr] = df.apply(lambda row : bin(row), axis=1)\n",
        "    print(f\"{ftr} Outliers Removed\")\n",
        "  print(\"\\n----------\\n\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,20))\n",
        "plt.suptitle(\"Box plot without Outliers\", fontsize=18, y=0.95)\n",
        "# Plot the all figures in loop with boxplot.\n",
        "for n,ticker in enumerate(col_list):\n",
        "  ax = plt.subplot(8,4, n+1)\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "  sns.boxplot(x=df[ticker], color= 'orange', ax=ax)\n",
        "\n",
        "  # Chart Formatting.\n",
        "  ax.set_title(ticker.upper())"
      ],
      "metadata": {
        "id": "xpiQwds4uh3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Box plot is a handy graphical depication for describing the behaviour of data in the middle and at the ends of distributions. The Box plot employs the median as well as the lower and upper quartiles(defined as 25th and 75th percentiles).if the lower quartile is Q1 and upper quartile is Q3, the difference (Q3-Q1) is known as the interquartile range, or IQ. A box plot is made by drawing a box between the higher and lower quartiles and a solid line across the box to find the median. The following quantities (referred to as fences) as required for recognising extreme values in the distribution's tails:\n",
        "\n",
        "\n",
        "lower fence: Q1 -1.5*IQ\n",
        "\n",
        "upper fence: Q3 +1.5*IQ"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the types of all features.\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no categorical features present in the dataset. So any categorical encoding does not required."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no text columns in the given dataset which we are working on.So,we are skipping this part."
      ],
      "metadata": {
        "id": "XzS03GgFzyD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Creating a column average building temperature based on all temperature.\n",
        "df['Average_building_Temperature'] = df[['temp_kitchen', 'temp_living_room', 'temp_Laundry_room', 'temp_office_room', 'temp_bathroom', 'temp_ironing_room', 'temp_teen_room', 'temp_parents_room']].mean(axis=1)\n",
        "# Creating a column of difference between outside and inside temperature.\n",
        "df['Temperture_difference'] = abs(df['Average_building_Temperature'] - df['temp_build_out'])\n",
        "\n",
        "# Creating a column average building humidity.\n",
        "df['Average_building_humidity'] = df[['hu_Kitchen', 'hu_living', 'hu_laundry', 'hu_office', 'hu_bath', 'hu_ironing_room', 'hu_teen', 'hu_parent']].mean(axis=1)\n",
        "# Creating a column of difference between outside and inside building humidity.\n",
        "df['Humidity_difference'] = abs(df['hu_build_out'] - df['Average_building_humidity'])"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we are going to drop two random variable column because they have no part in energy prediction.After that we will chek multicolinearity among remaining columns."
      ],
      "metadata": {
        "id": "nF6EztXl4pT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping rv1 and rv2.\n",
        "df.drop('rv1', axis=1, inplace= True)\n",
        "df.drop('rv2', axis=1, inplace= True)"
      ],
      "metadata": {
        "id": "fP1sIqCw5isk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to check the multicolinearity.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "  # Calculating VIF\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"variables\"] = X.columns\n",
        "  vif[\"VIF\"] = [round(variance_inflation_factor(X.values,i),2) for i in range(X.shape[1])]\n",
        "\n",
        "  return(vif)"
      ],
      "metadata": {
        "id": "kzvNf55859jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking multicolinearity.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Appliances']]]).sort_values(by='VIF',ascending= False)"
      ],
      "metadata": {
        "id": "UrR7nlsm7UxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its look like lots of column is a replica of one another. so we are going to remove atlest half of them."
      ],
      "metadata": {
        "id": "iOTrRIsj8M9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check multicolinearity.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Appliances','lights','temp_kitchen','temp_living_room','temp_Laundry_room','temp_office_room','temp_bathroom','hu_Kitchen','hu_living','hu_laundry','hu_office','hu_bath','temp_build_out','Average_building_Temperature']]]).sort_values(by= 'VIF', ascending=False)"
      ],
      "metadata": {
        "id": "6wohie1HY1gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets Remove, lights, temp_parents_room, Average_building_humidity, temp_ironing_room, out_humidity and check again."
      ],
      "metadata": {
        "id": "GYRFxzYtauNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check multicolinearity.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Appliances','lights','temp_kitchen','temp_living_room','temp_Laundry_room','temp_office_room','temp_bathroom','hu_Kitchen','hu_living','hu_laundry','hu_office','hu_bath','temp_build_out','Average_building_Temperature','Press_mm_hg','temp_parents_room','Average_building_humidity','temp_ironing_room','out_humidity']]]).sort_values(by='VIF',ascending=False)\n"
      ],
      "metadata": {
        "id": "a_K29YS7bQ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Remove hu_teen, hu_parents, temp_teen_room, hu_ironing_room, and check again."
      ],
      "metadata": {
        "id": "ktUdjFTrgHSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check multicolinearity.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Appliances','lights','temp_kitchen','temp_living_room','temp_Laundry_room','temp_office_room','temp_bathroom','hu_Kitchen','hu_living','hu_laundry','hu_office','hu_bath','temp_build_out','Average_building_Temperature','Press_mm_hg','temp_parents_room','Average_building_humidity','temp_ironing_room','out_humidity','hu_teen','hu_parent','temp_teen_room','hu_ironing_room']]]).sort_values(by='VIF',ascending=False)"
      ],
      "metadata": {
        "id": "T-QkPK7vgl6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now remove Temperature difference and check again.."
      ],
      "metadata": {
        "id": "5w1OkybFh-dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check multicolinearity.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Appliances','lights','temp_kitchen','temp_living_room','temp_Laundry_room','temp_office_room','temp_bathroom','hu_Kitchen','hu_living','hu_laundry','hu_office','hu_bath','temp_build_out','Average_building_Temperature','Press_mm_hg','temp_parents_room','Average_building_humidity','temp_ironing_room','out_humidity','hu_teen','hu_parent','temp_teen_room','hu_ironing_room','Temperature_difference','Visibility']]]).sort_values(by='VIF',ascending=False)"
      ],
      "metadata": {
        "id": "2wH6ZMyUiHVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now remove temp_out and visibility and check again."
      ],
      "metadata": {
        "id": "3gMpZv8Gi54W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check multicolinearity.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Appliances','lights','temp_kitchen','temp_living_room','temp_Laundry_room','temp_office_room','temp_bathroom','hu_Kitchen','hu_living','hu_laundry','hu_office','hu_bath','temp_build_out','Average_building_Temperature','Press_mm_hg','temp_parents_room','Average_building_humidity','temp_ironing_room','out_humidity','hu_teen','hu_parent','temp_teen_room','hu_ironing_room','Temperature_difference','temp_out','Visibility']]]).sort_values(by='VIF',ascending=False)"
      ],
      "metadata": {
        "id": "r846oRP2jFgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "df_removed = df[[i for i in df.describe().columns if i not in ['lights','temp_kitchen','temp_living_room','temp_Laundry_room','temp_office_room','temp_bathroom','hu_Kitchen','hu_living','hu_laundry','hu_office','hu_bath','temp_build_out','Average_building_Temperature','Press_mm_hg','temp_parents_room','Average_building_humidity','temp_ironing_room','out_humidity','hu_teen','hu_parent','temp_teen_room','hu_ironing_room','Temperature_difference','temp_out','Visibility']]]\n",
        "\n",
        "df_removed.head()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Variance Inflation Factor(VIF) for feature selection.\n",
        "\n",
        "The Variance Inflation Factor(VIF) is used to detect multicolinearity. Variance Inflation Factor(VIF) quantify how much the variance of predicted regression coefficients is inflated when the predictor variables are not linearly connected."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a multivariate regression model, multicolinearity exists when there is a correlation between many independent variables. Under ideal conditions, small VIF values, such as VIF< 10, suggest low correlation across variables. the VIF cutoff value is set to 10. Only variables having a VIF less than 10 will be included in the model.\n",
        "\n",
        "According to above stated criteria, we have removed column one by one and check whether all the features VIF value is less than 10 or not. Once all the VIF value of features is below 10 then we stop the checking multicolinearity. Important features for project that we have found are Appliances hu_build_out, Windspeed, Tdewpoint, rv1, month, weekday, hour, Humidity_difference."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution of all independent features.\n",
        "for col in df_removed.describe().columns:\n",
        "  fig= plt.figure(figsize=(9,6))\n",
        "  ax=fig.gca()\n",
        "  feature = (df_removed[col])\n",
        "  sns.distplot(df_removed[col])\n",
        "  ax.axvline(feature.mean(),color='magenta',linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HcVm5uLuvyJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "df_removed['Appliances']= df_removed['Appliances'].apply(lambda x: np.log10(x+1))\n",
        "df_removed['Windspeed'] = df_removed['Windspeed'].apply(lambda x: np.log10(x+1))"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, We think Appliances and Windspeed feature of the data need to be transformed, because they are skewed. We have use log transformation with them. Other feature followed normal distribution, symmetrical around y axis and some of them does not have any resemblace with normal distribution. They will be scalled in upcoming part."
      ],
      "metadata": {
        "id": "4nG-34aVyXmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution of the feature after transformation.\n",
        "for col in df_removed.describe().columns:\n",
        "  fig= plt.figure(figsize=(9,6))\n",
        "  ax= fig.gca()\n",
        "  feature = (df_removed[col])\n",
        "  sns.distplot(df_removed[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(), color='cyan', linestyle = 'dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pB-E3dzFzZXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.   Dimesionality Reduction"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "srhzBcui1IKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimesionality reduction, to the best of our knowledge, is not required for this dataset.\n",
        "\n",
        "Essentially, where high dimensions are a problem or a specific point in the algorithm of dimension reduction.In this data set no such problem is present according to our knowledge."
      ],
      "metadata": {
        "id": "KQqn5N9q1ZaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.   Data Splitting"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. choose splitting ratio wisely.\n",
        "X = df_removed.drop('Appliances', axis=1)\n",
        "y = df_removed['Appliances']\n",
        "X_train, X_test, y_train, y_test_ = train_test_split(X,y,test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "FjfazTld1MhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data spliting ration have you used and why ?"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code divides the data into training and testing sets using an 80:20 ratio. This indicates that 80% of the data is utilised to train the model while 20% is used to test the model.\n",
        "\n",
        "The ratio is determined by a number of parameters, including the size of the dataset, the complexity of the model, and the amount of computational resources available. In general, a larger training set produce a stronger model, but smaller testing set produces a higher variance in performance measurements. An 80:20 ratio is an excellent choice since it balances the trade off between having enough data to train the model and having enough data to evaluate the model performance."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.    Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "DvxHcbv86nLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, this dataset is not imbalanced."
      ],
      "metadata": {
        "id": "wlHaodyh6tTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "29S9_80s7FEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "2IYU3zSJ7LLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Scaling"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data.\n",
        "scaler= MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled_ = scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale your data and why ?"
      ],
      "metadata": {
        "id": "23sC8Fgv8MGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is scaled using Min-Max scaling or normalisation, which scales the data to a range between 0 and 1.\n",
        "\n",
        "This approch was chosen because it preserves the distribution of the data and the relationship between the features. Further more, many machine learning algorithms perform better with scaled data, particularly those that use distance based metrics or optimisation functions. Scaling was done with scikit-learns MinMaxscaler function."
      ],
      "metadata": {
        "id": "7lkyTI-28ggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sepearate two rows from test set to check it with best model.\n",
        "X_test_best = X_test_scaled_[-2:]\n",
        "y_test_best = y_test_[-2:]\n",
        "X_test_scaled = X_test_scaled_[:-2]\n",
        "y_test = y_test_[:-2]"
      ],
      "metadata": {
        "id": "P1akrYv6-E5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of all sepearate variables.\n",
        "print(X_test_best.shape)\n",
        "print(y_test_best.shape)\n",
        "print(X_test_scaled.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "7i_iG3lj_cUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_scaled = X_test_scaled[:-2]\n",
        "x_test_scaled.shape"
      ],
      "metadata": {
        "id": "th0YAtaeAF9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1  - Linear Regression."
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Linear Regression.\n",
        "lr = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lr.fit(X_train_scaled,y_train)\n",
        "# Predict the target values of train data.\n",
        "lr_train = lr.predict(X_train_scaled)\n",
        "# Predict the target value of test data.\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model using.\n",
        "mse_lr_train =mean_squared_error(y_train,lr_train)\n",
        "mse_lr_test = mean_squared_error(y_test,y_pred_lr)\n",
        "r2_lr_train = r2_score(y_train, lr_train)\n",
        "r2_lr_test = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(\"LinearRegression Mean Squared Error: \", mse_lr_test)\n",
        "print(\"LinearRegression R^2 Score: \", r2_lr_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge.\n",
        "ridge = Ridge()\n",
        "# Fit the Ridge model.\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "# Predict the target values of train data.\n",
        "r_train = ridge.predict(X_train_scaled)\n",
        "# Predict the target values of test data.\n",
        "y_pred_r = ridge.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model using metrics.\n",
        "mse_r_train = mean_squared_error(y_train, r_train)\n",
        "mse_r_test = mean_squared_error(y_test, y_pred_r)\n",
        "r2_r_train = r2_score(y_train, r_train)\n",
        "r2_r_test = r2_score(y_test, y_pred_r)\n",
        "\n",
        "print(\"Ridge Mean Squared Error:\", mse_r_test)\n",
        "print(\"Ridge R^2 Score:\", r2_r_test)"
      ],
      "metadata": {
        "id": "uvBYgQ7hmwar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso.\n",
        "lasso = Lasso()\n",
        "# Fit the Lasso model on the train model.\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "# Predict the target value of train data.\n",
        "l_train = lasso.predict(X_train_scaled)\n",
        "# Predict the target value of test data.\n",
        "y_pred_l = lasso.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model using metrics.\n",
        "mse_l_train = mean_squared_error(y_train, l_train)\n",
        "mse_l_test = mean_squared_error(y_test, y_pred_l)\n",
        "r2_l_train = r2_score(y_train, l_train)\n",
        "r2_l_test = r2_score(y_test, y_pred_l)\n",
        "\n",
        "print(\"Lasso Mean Squared Error:\", mse_l_test)\n",
        "print(\"Lasso R^2 Score:\", r2_l_test)"
      ],
      "metadata": {
        "id": "bzJG2nappgh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients.\n",
        "print(\"The Coefficients Obtain from LinearRegression model\", lr.coef_)\n",
        "print(\"The Coefficients Obtain from Ridge model\", ridge.coef_)\n",
        "print(\"The Coefficients Obtain from Lasso model\", lasso.coef_)\n"
      ],
      "metadata": {
        "id": "dypm_rZNzplt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cheecking the intercepts.\n",
        "print(\"The Intercepts Obtain from LinarRegression model\", lr.intercept_)\n",
        "print(\"The Intercepts Obtain from Ridge model\", ridge.intercept_)\n",
        "print(\"The Intercepts Obtain from Lasso model\", lasso.intercept_)"
      ],
      "metadata": {
        "id": "Ekk8ADVc0blP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Plotting the Predicted and Actual values.\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(((y_pred_lr)[500:550]),color= 'indigo')\n",
        "plt.plot(((y_pred_r)[500:550]), color= 'green')\n",
        "plt.plot(((y_pred_l)[500:550]), color = 'blue')\n",
        "plt.plot((np.array((y_test)[500:550])), color = 'red')\n",
        "plt.legend([\"LinearRegression\",\"Ridge\",\"Lasso\",\"Actual\"])\n",
        "plt.title(\"Sample of Actual V/S Predict of LinearRegression\" )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "ridge_params = {'alpha': [0.001, 0.01, 0.1, 0.5, 1, 2, 5]}\n",
        "lasso_params = {'alpha' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1]}\n",
        "\n",
        "# Creating Ridge and Lasso regression objects.\n",
        "ridge = Ridge()\n",
        "lasso = Lasso()\n",
        "\n",
        "# Creating GridSearchCV.\n",
        "ridge_cv = GridSearchCV(ridge, param_grid= ridge_params, scoring= 'neg_mean_squared_error')\n",
        "lasso_cv = GridSearchCV(lasso, param_grid= lasso_params, scoring= 'neg_mean_squared_error')\n",
        "\n",
        "# Fit the Model using GridSerchCV.\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Getting the best hyperparameter and fit the model again using the best hyperparameters.\n",
        "ridge_best = Ridge(alpha=ridge_cv.best_params_['alpha']).fit(X_train_scaled, y_train)\n",
        "lasso_best = Lasso(alpha=lasso_cv.best_params_['alpha']).fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict train data using the best model.\n",
        "y_train_ridge = ridge_best.predict(X_train_scaled)\n",
        "y_train_lasso = lasso_best.predict(X_train_scaled)\n",
        "\n",
        "# Predict test data using the best model.\n",
        "y_pred_ridge = ridge_best.predict(X_test_scaled)\n",
        "y_pred_lasso = lasso_best.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mse_ridge_train = mean_squared_error(y_train, y_train_ridge)\n",
        "mse_ridge_test = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge_train = r2_score(y_train, y_train_ridge)\n",
        "r2_ridge_test = r2_score(y_test, y_pred_ridge)\n",
        "mse_lasso_train = mean_squared_error(y_train, y_train_lasso)\n",
        "mse_lasso_test = mean_squared_error(y_test, y_pred_lasso)\n",
        "r2_lasso_train = r2_score(y_train, y_train_lasso)\n",
        "r2_lasso_test = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "# Print the evaluation metrics for both models.\n",
        "print(\"Ridge Regression - Best alpha:\", ridge_cv.best_params_['alpha'])\n",
        "print(\"Ridge Mean squared Error:\", (mse_ridge_test))\n",
        "print(\"Ridge R^2 Score:\", (r2_ridge_test))\n",
        "mse_percent_ridge = mse_ridge_test * 100\n",
        "r2_percent_ridge = r2_ridge_test * 100\n",
        "print(\"Ridge - Mean Squared Error: {:.2f}%\".format(mse_percent_ridge))\n",
        "print(\"Ridge - R-Squared :{:.2f}%\".format(r2_percent_ridge))\n",
        "\n",
        "\n",
        "print(\"\\nLasso Regression - Best alpha:\", lasso_cv.best_params_['alpha'])\n",
        "print(\"Lasso Mean Squared Error\", (mse_lasso_test))\n",
        "print(\"Lasso R^2 Score:\",(r2_lasso_test))\n",
        "mse_percent_lasso = mse_lasso_test * 100\n",
        "r2_percent_lasso = r2_lasso_test * 100\n",
        "print(\"Lasso - Mean Squared Error: {:.2f}%\".format(mse_percent_lasso))\n",
        "print(\"Lasso - R-Score :{:.2f}%\".format(r2_percent_lasso))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of metric score of linear regression.\n",
        "list_lr =['LinearRegression', mse_l_train,mse_l_test,r2_l_train,r2_l_test]\n",
        "# Creating a empty dataframe for metric score columns.\n",
        "score = pd.DataFrame(columns= ['Model', 'Train MSE', 'Test MSE', 'Train R2_Score', 'Test R2_Score'])\n",
        "# Adding the Rows to data frame of linear regression.\n",
        "score.loc[len(score)] = list_lr\n",
        "\n",
        "# Creating a list of Ridge metric score after hyperparameter tuning.\n",
        "list_ridge = ['Ridge', mse_ridge_train,mse_ridge_test,r2_ridge_train,r2_ridge_test]\n",
        "# Adding the Rows to dataframe.\n",
        "score.loc[len(score)] = list_ridge\n",
        "\n",
        "# Creating a list of Lasso metric score after hyperparameter tuning .\n",
        "list_lasso = ['Lasso', mse_lasso_train,mse_lasso_test,r2_lasso_train,r2_lasso_test]\n",
        "# Adding the Rows to dataframe.\n",
        "score.loc[len(score)] = list_lasso\n",
        "print(score)"
      ],
      "metadata": {
        "id": "7-fQFV81TyGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, the gridSearchCV hyperperameter optimisation technique was employed.GridSearchCV was chosen because it throughly searches through a specific hyperparameter space to discover the ideal hyperparameter that would result in the greatest model performance. it is frequently used method for hyperparameter optimisation that assures that the optimal hyperparameters are identified within the defined parameter space."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, following hyperparameter optimisation, we can see some improvement in the Ridge Regression model.The MSE reduced from 0.039059 to 0.039057, but the R-Squred climbed 0.162756 to 0.162799.\n",
        "\n",
        "However, following hyperparameter optimisation, the MSE of Lasso regression model decrease from 0.04665 to 0.039067. The R-Squred score, on the other hand increased from -0.00013 to 0.162604."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2  Decision Tree Regression."
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Define the Decision tree model.\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Fit the model.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions on the test set.\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_DT = mean_squared_error(y_test, y_pred)\n",
        "r2_DT = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean squared error: \", mse_DT)\n",
        "print(\"R-squared:\", r2_DT)"
      ],
      "metadata": {
        "id": "5d9gOsjXROM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.regplot(x=y_test, y=y_pred, color = 'green')\n",
        "plt.xlabel('True values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Hyperparameters.\n",
        "h_param = {'max_depth':[None, 15, 20,25],\n",
        "           'min_samples_split' : [20, 25, 30],\n",
        "           'min_samples_leaf' : [4, 8, 12]}\n",
        "\n",
        "# GridSerchCV\n",
        "grid_search = GridSearchCV(model, h_param, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "print('Best Parameters :', grid_search.best_params_)\n",
        "\n",
        "dt_best = DecisionTreeRegressor(max_depth = grid_search.best_params_['max_depth'],\n",
        "                                min_samples_split = grid_search.best_params_['min_samples_split'],\n",
        "                                min_samples_leaf= grid_search.best_params_['min_samples_leaf'],\n",
        "                                random_state=0)\n",
        "# Fit the Algorithm\n",
        "dt_best.fit(X_train_scaled, y_train)\n",
        "# Predict on the training model.\n",
        "dtr_train = dt_best.predict(X_train_scaled)\n",
        "# predict on the model.\n",
        "y_pred = dt_best.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_dsT_train = mean_squared_error(y_train, dtr_train)\n",
        "mse_dsT_test = mean_squared_error(y_test, y_pred)\n",
        "r2_dsT_train = r2_score(y_train, dtr_train)\n",
        "r2_dsT_test = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean squared error: \", mse_dsT_test)\n",
        "print(\"R-squared: \", r2_dsT_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list od Decision Tree Regression metric score.\n",
        "dtr_list = ['DecisionTree', mse_dsT_train,mse_dsT_test,r2_dsT_train,r2_dsT_test]\n",
        "# Adding the rows by list.\n",
        "score.loc[len(score)]=dtr_list\n",
        "score"
      ],
      "metadata": {
        "id": "d4xHNwZxIU5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code,we utilised the GridSearchCV hyperparameter optimisation approach.GridSearchCV was chosen because it thoroughly searches through a specific hyperparameter space to discover the  ideal hyperparameter that result in the greatest model performance. it is frequently used method for hyperparameter optimisation that ensure the optimal hyperparameters are discovered within the defined parameter space."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the evalution metric improve after adopting hyperparameter optimisation approach.The R-squared value has grown from 0.5210 to 0.5823, while the Mean squared error has dropped from 0.02234 to 0.01948."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3  Random Forest Regression."
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "# Fit the Algorithm\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "# Predict on the model\n",
        "y_pred = rf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_rf = mean_squared_error(y_test, y_pred)\n",
        "r2_rf = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean squared error: \", mse_rf)\n",
        "print(\"R-squared: \", r2_rf)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.title('Random Forest Regressor - Actual V/s Predicted')\n",
        "sns.regplot(x=y_test, y=y_pred, scatter=False, color= 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "param_grid = {'n_estimators': [350, 420],\n",
        "              'max_depth': [None, 1,7],\n",
        "              'min_samples_leaf':[1,7],\n",
        "              'max_features':['sqrt','log2']}\n",
        "\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring= 'neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "# Fit the Algorithm\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "best_rf_model.fit(X_train_scaled, y_train)\n",
        "# Predict on the train model\n",
        "rf_train = best_rf_model.predict(X_train_scaled)\n",
        "# Predict on Test model.\n",
        "y_pred = best_rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate te metric score.\n",
        "mse_rf_train = mean_squared_error(y_train, rf_train)\n",
        "mse_rf_test = mean_squared_error(y_test, y_pred)\n",
        "r2_rf_train = r2_score(y_train, rf_train)\n",
        "r2_rf_test = r2_score(y_test, y_pred)\n",
        "\n",
        "print('Mean Squared Error :', mse_rf_test)\n",
        "print('R2 Squared :', r2_rf_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of random forest regressor metric score.\n",
        "rf_list = ['Randomforest', mse_rf_train,mse_rf_test,r2_rf_train,r2_rf_test]\n",
        "# Add the row by list.\n",
        "score.loc[len(score)]=rf_list\n",
        "score"
      ],
      "metadata": {
        "id": "XUXGgLTUWmOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we used GridSearchCV hyperparameter optimization approch. GridsearchCV was chosen because it searches extensively over a specific hyperparameter space.to discove the optimal hyperparameters that result in the greates model performance. it is popular method for optimising hyperparameter since it assures that the optimal hyperparameters are identified within the specified parameter space."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a very small improvement in the evaluation metrics after implementing hyperparameter optimization techniques.The Mean squared error has decreased from 0.013163 to 0.012858 and, R-squared value has increased from 0.71783 to 0.72439."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4  Extra Tree Regression."
      ],
      "metadata": {
        "id": "fQTMCOenaoFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "# Define the Extra tree regression model.\n",
        "model = ExtraTreesRegressor(random_state=42)\n",
        "# Fit the model.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the model on test set.\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_br = mean_squared_error(y_test, y_pred)\n",
        "r2_br = r2_score(y_test, y_pred)\n",
        "\n",
        "print('Mean squared error :', mse_br)\n",
        "print('R-squared :', r2_br)"
      ],
      "metadata": {
        "id": "1ssX8D2ea3gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "3-gXl5f6cmKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.title('Extra Tree Regressor - Actual V/s Predicted')\n",
        "sns.regplot(x=y_test, y=y_pred, scatter=False, color= 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PGRQDIucrhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "UzIuBym6dzDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Hyperparameter.\n",
        "h_param = {'bootstrap': [True, False],\n",
        "           'max_depth': [70,100,None],\n",
        "           'criterion': ['squared_error'],\n",
        "           'max_features': ['log2', 'sqrt'],\n",
        "           'n_estimators':[10,1400,100]}\n",
        "\n",
        "# GridserchCV\n",
        "grid_search = GridSearchCV(model, h_param, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "print('Best Parameters', grid_search.best_params_)\n",
        "\n",
        "br_best = grid_search.best_estimator_\n",
        "\n",
        "# Fit the Algorithm.\n",
        "br_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on training model.\n",
        "br_train = br_best.predict(X_train_scaled)\n",
        "\n",
        "# predict on test model.\n",
        "y_pred = br_best.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_br_train = mean_squared_error(y_train, br_train)\n",
        "mse_br_test = mean_squared_error(y_test, y_pred)\n",
        "r2_br_train = r2_score(y_train, br_train)\n",
        "r2_br_test = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean squared error :\", mse_br_test)\n",
        "print(\"R-squared :\", r2_br_test)"
      ],
      "metadata": {
        "id": "X-soxv-Fd0bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of Extra Tree regressor metric score.\n",
        "br_list = ['ExtraTreeRegressor', mse_br_train,mse_br_test,r2_br_train,r2_br_test]\n",
        "# Add the Rows by list.\n",
        "score.loc[len(score)]=br_list\n",
        "score"
      ],
      "metadata": {
        "id": "_iyI8GI2EoUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "scxsCtHEFbHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we used GridSearchCV hyperparameter optimization approch. GridsearchCV was chosen because it searches extensively over a specific hyperparameter space.to discove the optimal hyperparameters that result in the greates model performance. it is popular method for optimising hyperparameter since it assures that the optimal hyperparameters are identified within the specified parameter space."
      ],
      "metadata": {
        "id": "qyVgoJpDFjkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "L3eppiIMFuuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a very small improvement in the evaluation metrics after implementing the hyperparameter optimization techniques.The mean squared error has decereased from 0.01229 to 0.011945 and the R-squared value has increased from 0.7364 to 0.7439."
      ],
      "metadata": {
        "id": "VDA1nv5UFv65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5   XGB Regression."
      ],
      "metadata": {
        "id": "1EIji8-kHGsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation\n",
        "xgb = XGBRegressor()\n",
        "\n",
        "# Fit the algorithm.\n",
        "xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the model.\n",
        "y_pred = xgb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_xgb = mean_squared_error(y_test, y_pred)\n",
        "r2_xgb = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean squared error:\", mse_xgb)\n",
        "print(\"R-Squared:\", r2_xgb)"
      ],
      "metadata": {
        "id": "qF1HMRbEHWCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "IjvrfocpIhVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.title('XGBRegressor - Actual V/s Predicted')\n",
        "sns.regplot(x= y_test, y= y_pred, scatter= False, color= 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LhnwEPygItNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "kcA9O-PlJf7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "param_grid = {'n_estimators' : [200,400,600,800],\n",
        "              'max_depth': [10,15,20],\n",
        "              'learning_rate' : [0.3, 0.5, 0.7],\n",
        "              'subsample': [1,2,4],\n",
        "              'colsample_bytree' : [1,5,10]}\n",
        "\n",
        "# GridsearchCV.\n",
        "grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring= 'neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Fit the model.\n",
        "best_xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the train model.\n",
        "xgb_train = best_xgb_model.predict(X_train_scaled)\n",
        "# Predict the test model.\n",
        "y_pred = best_xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the metric score.\n",
        "mse_xgb_train = mean_squared_error(y_train, xgb_train)\n",
        "mse_xgb_test = mean_squared_error(y_test, y_pred)\n",
        "r2_xgb_train = r2_score(y_train, xgb_train)\n",
        "r2_xgb_test = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean squared error :\", mse_xgb_test)\n",
        "print(\"R-Socre :\", r2_xgb_test)\n"
      ],
      "metadata": {
        "id": "nTk13dE2Jlvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of XGBRegressor metric score.\n",
        "xgb_list = ['XGBRegressor', mse_xgb_train,mse_xgb_test,r2_xgb_train,r2_xgb_test]\n",
        "# Adding Rows by list.\n",
        "score.loc[len(score)]=xgb_list\n",
        "score"
      ],
      "metadata": {
        "id": "kDNoG9aoPHvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "Y2jgAdQJP_IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we used GridSearchCV hyperparameter optimization approch. GridsearchCV was chosen because it searches extensively over a specific hyperparameter space.to discove the optimal hyperparameters that result in the greates model performance. it is popular method for optimising hyperparameter since it assures that the optimal hyperparameters are identified within the specified parameter space."
      ],
      "metadata": {
        "id": "68iLmpoqQALh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "YFQuJfvlQL-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a very small improvement in the evaluation metrics after implementing the hyperparameter optimization techniques.The mean squared error has decereased from  0.01757 to \t0.01431 and the R-squared value has increased from 0.62330 to 0.69313."
      ],
      "metadata": {
        "id": "JlzypYDJQR8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6   LGBM Regression."
      ],
      "metadata": {
        "id": "LHwIORQERTVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Implementation\n",
        "lgbm = LGBMRegressor(random_state=0)\n",
        "\n",
        "# Fit the Algorithm.\n",
        "lgbm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the model.\n",
        "y_pred = lgbm.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_lgbm = mean_squared_error(y_test, y_pred)\n",
        "r2_lgbm = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean squared error:\" , mse_lgbm)\n",
        "print(\"R-Squared:\", r2_lgbm)"
      ],
      "metadata": {
        "id": "R1lZ1ikxRZ5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "RD5ED2KASkFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.title('LGBMRegressor  - Actual V/s Predicted')\n",
        "sns.regplot(x= y_test, y= y_pred, scatter= False, color= 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-TbEUpfvSq5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "oOLH_PdaTe5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "param_grid = {'learning_rate': [0.1, 0.05],\n",
        "              'n_estimators': [50, 100, 150],\n",
        "              'max_depth' : [6,8],\n",
        "              'colsample_bytree': [0.1, 0.8, 1],\n",
        "              'subsample': [0.7,0.8,0.9],\n",
        "              'min_child_samples': [1,5,10]}\n",
        "\n",
        "# GridsearchCV.\n",
        "grid_search = GridSearchCV(lgbm, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "print('Best Parameters :', grid_search.best_params_)\n",
        "\n",
        "# Fit the Algorithm.\n",
        "best_lgbm_model = grid_search.best_estimator_\n",
        "best_lgbm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the train model.\n",
        "lgbm_train = best_lgbm_model.predict(X_train_scaled)\n",
        "# Predict the test model.\n",
        "y_pred = best_lgbm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the metric score.\n",
        "mse_lgbm_train = mean_squared_error(y_train, lgbm_train)\n",
        "mse_lgbm_test = mean_squared_error(y_test, y_pred)\n",
        "r2_lgbm_train = r2_score(y_train, lgbm_train)\n",
        "r2_lgbm_test = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse_lgbm_test)\n",
        "print(\"R-Score :\", r2_lgbm_test)\n"
      ],
      "metadata": {
        "id": "dlc--4uETksD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of LGBMRegressor metric score.\n",
        "lgbm_list= ['LGBMRegressor', mse_lgbm_train,mse_lgbm_test,r2_lgbm_train,r2_lgbm_test]\n",
        "# Add Row by list.\n",
        "score.loc[len(score)]= lgbm_list\n",
        "score"
      ],
      "metadata": {
        "id": "8JiJxWGSYUg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "dDrpu6JFZA9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we used GridSearchCV hyperparameter optimization approch. GridsearchCV was chosen because it searches extensively over a specific hyperparameter space.to discove the optimal hyperparameters that result in the greates model performance. it is popular method for optimising hyperparameter since it assures that the optimal hyperparameters are identified within the specified parameter space."
      ],
      "metadata": {
        "id": "g8N-lGIwZGhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ZDgeF8fpZNqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a very small improvement in the evaluation metrics after implementing the hyperparameter optimization techniques.The mean squared error has decereased from  0.01982 to 0.01911 and the R-squared value has increased from  0.57501 to 0.59031."
      ],
      "metadata": {
        "id": "a54gCnKLZTHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 7   Stacking Regression."
      ],
      "metadata": {
        "id": "ZqP-Flv2aNss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 7 Implementation\n",
        "estimators = [('xgb', XGBRegressor()),\n",
        "              ('etr', ExtraTreesRegressor()),\n",
        "              ('random', RandomForestRegressor())]\n",
        "\n",
        "sr = StackingRegressor(estimators=estimators,\n",
        "                       final_estimator= ExtraTreesRegressor(n_estimators=1400, random_state=42), cv=5)\n",
        "\n",
        "# Fit the Algorithm.\n",
        "sr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the train model.\n",
        "sr_train = sr.predict(X_train_scaled)\n",
        "# Predict the Test model.\n",
        "y_pred = sr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model.\n",
        "mse_sr_train  = mean_squared_error(y_train, sr_train)\n",
        "mse_sr_test = mean_squared_error(y_test, y_pred)\n",
        "r2_sr_train = r2_score(y_train, sr_train)\n",
        "r2_sr_test = r2_score(y_test, y_pred)\n",
        "\n",
        "print('Mean Squared error:', mse_sr_test)\n",
        "print('R-Squared :', r2_sr_test)"
      ],
      "metadata": {
        "id": "j0FDrgbYaeL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "kxanZL3sYYo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.title('StackingRegressor  - Actual V/s Predicted')\n",
        "sns.regplot(x= y_test, y=y_pred, scatter= False, color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9efqHP5TYmS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of Stacking regressor metric score.\n",
        "sr_list = ['StackingRegressor', mse_sr_train,mse_sr_test,r2_sr_train,r2_sr_test]\n",
        "# Adding the row by list.\n",
        "score.loc[len(score)]=sr_list\n",
        "score"
      ],
      "metadata": {
        "id": "AL05Hw4UZuxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In these circumstances, the evaluation metrics we examined for a good business impact were as follows:\n",
        "\n",
        "**Mean Squared Error(MSE) and **\n",
        "\n",
        "**The R2 Score**\n",
        "\n",
        "MSE is a measure of the average squared difference between the actual and anticipated values of the target variable. A lower MSE suggest that the model performed better in terms of prediction accuracy.\n",
        "\n",
        "R2 Score is a statistical measure that shows the proportion of the variance in the target variable that can be explained by the independet variable. it indicates the models goodness of fit. A higher R2 score indicates a better fit of the model to the data and better prediction abilities."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax= plt.subplots(1,2,figsize=(15,10))\n",
        "# plot the mse of all model.\n",
        "score.plot(x=\"Model\", y=['Train MSE', 'Test MSE'], kind=\"bar\", title= \"MSE Score Results\", ax=ax[0])\n",
        "# plot the r2 score of all model.\n",
        "score.plot(x=\"Model\", y=['Train R2_Score', 'Test R2_Score'], kind=\"bar\", title = 'R2 Score Results', ax=ax[1])"
      ],
      "metadata": {
        "id": "9aihk6Qneg3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The ExtraTreeRegressor MODEL outperformed the other three models in terms of MEAN SQUARED ERROR and R-SQUARED value, with the lowest MSE and highest R-SQUARED value.**\n",
        "\n",
        "**As a result, we go with the ExtraTreeRegressor model as the final prediction model.**"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ExtraTreeRegressor model was chosen as the final model, which is an ensemble learning method that mixes numerous decision Trees to improve forecast accuracy and prevent overfitting. it works by training a large number of decision tree and then outputs the class that is the mode of the classes(classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "There are some fantastic libraries like LIME, SHAPE, ELI5 etc to make thing interpretable we will make the use of ELI5 to understand our mode."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install ELI5\n",
        "! pip install eli5"
      ],
      "metadata": {
        "id": "CSXljujpo5a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import eli5.\n",
        "import eli5 as eli\n",
        "# Explain the weights.\n",
        "eli.explain_weights(br_best, feature_names = list(X.columns))"
      ],
      "metadata": {
        "id": "Lvy_x3pjpOMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above figure, we can conclude that the hour feature is most important for the prediction of appliance energy, followed by Tdewpoint and hu_build_out and the least important"
      ],
      "metadata": {
        "id": "l0TKm1tLqH0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(br_best, open(filename,'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "y_pred_best = loaded_model.predict(X_test_best)\n",
        "\n",
        "print(y_pred_best)\n",
        "print(y_test_best)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Linear Regression, Lasso Regression, Ridge Regression, Decision Tree Regression, Random Forest Regression, XGBoost, Light Gradient Boosting Regressor, Stacking Regressor are used to predict household appliance energy consumption.\n",
        "\n",
        "* We compare and evaluate the best model with lowest error and the greatest R-Squared Score after suitable preprocessing and fitting the fourteen models.\n",
        "\n",
        "* temp_build_out and temp_out, temp_parents_room and temp_ironing_room had a high association with each other, thus we deleted temp_build_out and temp_parent_room. When examining the influence of the Random variable attribute, the linear models assigned the Random variable near zero weights, negating its influence in predicting the target variable.\n",
        "\n",
        "* Inside room temperature and humidity has high variance inflation factor (VIF), so we have to drop them.\n",
        "\n",
        "* Light consumption was evaluated highly when all factors were used. When evaluatting different predictor subgroups, elimainating light usage did not appear to have a substantial influence. This could indicate that other characteristic are well connected with light energy usage.\n",
        "\n",
        "* The pressure's significant prediction power could be attributed to its influence on wind speed and higher rainfall probability, which could potentially raise the occupancy of the residence.\n",
        "\n",
        "* Because this dataset involves a temporal component, we believe that employing temporal series Analysis techniques will result in superior result.\n",
        "\n",
        "* ExtraTreeRegressor was discovered to be the best performing model with R-squared value of 0.7439\n",
        "\n",
        "* Appliances hu_build_out, Windspeed, Tdewpoint, month, weekday,hour,Humidity_difference all play a part in energy prediction.\n",
        "\n",
        "\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}